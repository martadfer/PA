{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bibliotecas pandas de leitura de arquivos\n",
    "import pandas as pd\n",
    "#biblioteca do kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "#avaliar classificacao cluster\n",
    "#from sklearn.metrics.cluster import adjusted_rand_score\n",
    "#visualizar graficos\n",
    "import matplotlib.pyplot as plt\n",
    "#caracter \n",
    "import re\n",
    "#countar os itens de uma lista\n",
    "from collections import Counter\n",
    "#separar treino, teste, validação\n",
    "from sklearn.model_selection import train_test_split\n",
    "#metricas\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### FUNÇÕES #####################################################################\n",
    "\n",
    "\n",
    "\n",
    "#Funções para calcular o melhor numero de cluster(grupos) que seu dataset gerar\n",
    "def calculate_wcss(data):\n",
    "    \"\"\"\n",
    "    Calcula a soma dos quadrados intra-clusters para 19\n",
    "    quantidades de clusters, iniciando com o mínimo de 2 clusters\n",
    "    \n",
    "    Parametros\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "        conjunto de dados para fazer o `.fit()` do KMeans\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    wcss : lista contendo os valores de soma de quadrados intra-cluster\n",
    "    \"\"\"\n",
    "    from sklearn.cluster import KMeans\n",
    "    wcss = []\n",
    "    for n in range(2, 21):\n",
    "        kmeans = KMeans(n_clusters=n)\n",
    "        kmeans.fit(X=data)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "    return wcss\n",
    "\n",
    "def optimal_number_of_clusters(wcss):\n",
    "    \"\"\"\n",
    "    Calcula a maior distância entre os pontos que marcam as \n",
    "    somas dos quadrados intra-clusters para 19 calculadas \n",
    "    com `calculate_wcss()`\n",
    "    \n",
    "    Parametros\n",
    "    ----------\n",
    "    wcss : lista\n",
    "        lista contendo os valores de soma de quadrados intra-cluster\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int : número de clusters \n",
    "    \"\"\"\n",
    "    from math import sqrt\n",
    "    x1, y1 = 2, wcss[0]\n",
    "    x2, y2 = 20, wcss[len(wcss)-1]\n",
    "\n",
    "    distances = []\n",
    "    for i in range(len(wcss)):\n",
    "        x0 = i+2\n",
    "        y0 = wcss[i]\n",
    "\n",
    "        numerator = abs((y2-y1)*x0 - (x2-x1)*y0 + x2*y1 - y2*x1)\n",
    "        denominator = sqrt((y2 - y1)**2 + (x2 - x1)**2)\n",
    "        distances.append(numerator/denominator)\n",
    "    return distances.index(max(distances)) + 2\n",
    "\n",
    "#convert peso em float\n",
    "def Tratar_Peso(peso):    \n",
    "    \n",
    "    try:\n",
    "        if peso[-1] == ',' or peso[-1] == '.':\n",
    "            peso =  peso[:-1]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        peso = float(peso)\n",
    "    except:\n",
    "        peso = float(peso.replace(',','.'))\n",
    "    return peso\n",
    "\n",
    "#convert altura\n",
    "def Tratar_Altura(altura):       \n",
    "       \n",
    "    \n",
    "    if re.match('\\d,,\\d', altura):\n",
    "        altura = altura.replace(',,','')\n",
    "    \n",
    "    try:\n",
    "        if altura[-1] == ',' or altura[-1] == '.' or  altura[-1] == ']':\n",
    "            altura =  altura[:-1]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        altura = float(altura)\n",
    "    except:\n",
    "        altura = float(altura.replace(',','.'))     \n",
    "    \n",
    "    # convt 150.00 para 1.50\n",
    "    if altura >= 3:\n",
    "        altura = altura/100 \n",
    "    \n",
    "    \n",
    "    return altura\n",
    "\n",
    "#calcular o IMC\n",
    "def IMC(peso,altura):  \n",
    "        \n",
    "    imc = float(peso) / (float(altura) * float(altura))\n",
    "    \n",
    "    return imc\n",
    "\n",
    "\n",
    "#Trocar  True para 1 e False para 0\n",
    "def verdadeiro_falso(valor):\n",
    "    if valor == True:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'classificados_por_padroes_de_doenca_estatistica.csv' does not exist: b'classificados_por_padroes_de_doenca_estatistica.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-aa683f6a031f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#leitura da base\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'classificados_por_padroes_de_doenca_estatistica.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\marta\\anaconda3\\envs\\emoti\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marta\\anaconda3\\envs\\emoti\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marta\\anaconda3\\envs\\emoti\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marta\\anaconda3\\envs\\emoti\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marta\\anaconda3\\envs\\emoti\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'classificados_por_padroes_de_doenca_estatistica.csv' does not exist: b'classificados_por_padroes_de_doenca_estatistica.csv'"
     ]
    }
   ],
   "source": [
    "#leitura da base\n",
    "dataset= pd.read_csv('classificados_por_padroes_de_doenca_estatistica.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paciente com altura ?\n",
    "dataset = dataset[dataset['altura']!= '?']\n",
    "#filtrar nulos em peso e altura\n",
    "dataset = dataset[~dataset['peso'].isnull() & ~dataset['altura'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converter Peso e Altura\n",
    "dataset['peso_tratada'] = dataset['peso'].apply(Tratar_Peso)\n",
    "dataset['altura_tratada'] = dataset['altura'].apply(Tratar_Altura)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculo do IMC\n",
    "dataset['IMC'] = dataset.apply(lambda row : IMC(row['peso_tratada'], row['altura_tratada']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sist e Diast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecionando variaveis sist 9:00 a diast 8:45\n",
    "#campos sist e diast\n",
    "ls_sist_diast = ['sist 9:00',\n",
    " 'sist 9:15',\n",
    " 'sist 9:30',\n",
    " 'sist 9:45',\n",
    " 'sist 10:00',\n",
    " 'sist 10:15',\n",
    " 'sist 10:30',\n",
    " 'sist 10:45',\n",
    " 'sist 11:00',\n",
    " 'sist 11:15',\n",
    " 'sist 11:30',\n",
    " 'sist 11:45',\n",
    " 'sist 12:00',\n",
    " 'sist 12:15',\n",
    " 'sist 12:30',\n",
    " 'sist 12:45',\n",
    " 'sist 13:00',\n",
    " 'sist 13:15',\n",
    " 'sist 13:30',\n",
    " 'sist 13:45',\n",
    " 'sist 14:00',\n",
    " 'sist 14:15',\n",
    " 'sist 14:30',\n",
    " 'sist 14:45',\n",
    " 'sist 15:00',\n",
    " 'sist 15:15',\n",
    " 'sist 15:30',\n",
    " 'sist 15:45',\n",
    " 'sist 16:00',\n",
    " 'sist 16:15',\n",
    " 'sist 16:30',\n",
    " 'sist 16:45',\n",
    " 'sist 17:00',\n",
    " 'sist 17:15',\n",
    " 'sist 17:30',\n",
    " 'sist 17:45',\n",
    " 'sist 18:00',\n",
    " 'sist 18:15',\n",
    " 'sist 18:30',\n",
    " 'sist 18:45',\n",
    " 'sist 19:00',\n",
    " 'sist 19:15',\n",
    " 'sist 19:30',\n",
    " 'sist 19:45',\n",
    " 'sist 20:00',\n",
    " 'sist 20:15',\n",
    " 'sist 20:30',\n",
    " 'sist 20:45',\n",
    " 'sist 21:00',\n",
    " 'sist 21:15',\n",
    " 'sist 21:30',\n",
    " 'sist 21:45',\n",
    " 'sist 22:00',\n",
    " 'sist 22:15',\n",
    " 'sist 22:30',\n",
    " 'sist 22:45',\n",
    " 'sist 23:00',\n",
    " 'sist 23:30',\n",
    " 'sist 0:00',\n",
    " 'sist 0:30',\n",
    " 'sist 1:00',\n",
    " 'sist 1:30',\n",
    " 'sist 2:00',\n",
    " 'sist 2:30',\n",
    " 'sist 3:00',\n",
    " 'sist 3:30',\n",
    " 'sist 4:00',\n",
    " 'sist 4:30',\n",
    " 'sist 5:00',\n",
    " 'sist 5:30',\n",
    " 'sist 6:00',\n",
    " 'sist 6:15',\n",
    " 'sist 6:30',\n",
    " 'sist 6:45',\n",
    " 'sist 7:00',\n",
    " 'sist 7:15',\n",
    " 'sist 7:30',\n",
    " 'sist 7:45',\n",
    " 'sist 8:00',\n",
    " 'sist 8:15',\n",
    " 'sist 8:30',\n",
    " 'sist 8:45',\n",
    " 'diast 9:00',\n",
    " 'diast 9:15',\n",
    " 'diast 9:30',\n",
    " 'diast 9:45',\n",
    " 'diast 10:00',\n",
    " 'diast 10:15',\n",
    " 'diast 10:30',\n",
    " 'diast 10:45',\n",
    " 'diast 11:00',\n",
    " 'diast 11:15',\n",
    " 'diast 11:30',\n",
    " 'diast 11:45',\n",
    " 'diast 12:00',\n",
    " 'diast 12:15',\n",
    " 'diast 12:30',\n",
    " 'diast 12:45',\n",
    " 'diast 13:00',\n",
    " 'diast 13:15',\n",
    " 'diast 13:30',\n",
    " 'diast 13:45',\n",
    " 'diast 14:00',\n",
    " 'diast 14:15',\n",
    " 'diast 14:30',\n",
    " 'diast 14:45',\n",
    " 'diast 15:00',\n",
    " 'diast 15:15',\n",
    " 'diast 15:30',\n",
    " 'diast 15:45',\n",
    " 'diast 16:00',\n",
    " 'diast 16:15',\n",
    " 'diast 16:30',\n",
    " 'diast 16:45',\n",
    " 'diast 17:00',\n",
    " 'diast 17:15',\n",
    " 'diast 17:30',\n",
    " 'diast 17:45',\n",
    " 'diast 18:00',\n",
    " 'diast 18:15',\n",
    " 'diast 18:30',\n",
    " 'diast 18:45',\n",
    " 'diast 19:00',\n",
    " 'diast 19:15',\n",
    " 'diast 19:30',\n",
    " 'diast 19:45',\n",
    " 'diast 20:00',\n",
    " 'diast 20:15',\n",
    " 'diast 20:30',\n",
    " 'diast 20:45',\n",
    " 'diast 21:00',\n",
    " 'diast 21:15',\n",
    " 'diast 21:30',\n",
    " 'diast 21:45',\n",
    " 'diast 22:00',\n",
    " 'diast 22:15',\n",
    " 'diast 22:30',\n",
    " 'diast 22:45',\n",
    " 'diast 23:00',\n",
    " 'diast 23:30',\n",
    " 'diast 0:00',\n",
    " 'diast 0:30',\n",
    " 'diast 1:00',\n",
    " 'diast 1:30',\n",
    " 'diast 2:00',\n",
    " 'diast 2:30',\n",
    " 'diast 3:00',\n",
    " 'diast 3:30',\n",
    " 'diast 4:00',\n",
    " 'diast 4:30',\n",
    " 'diast 5:00',\n",
    " 'diast 5:30',\n",
    " 'diast 6:00',\n",
    " 'diast 6:15',\n",
    " 'diast 6:30',\n",
    " 'diast 6:45',\n",
    " 'diast 7:00',\n",
    " 'diast 7:15',\n",
    " 'diast 7:30',\n",
    " 'diast 7:45',\n",
    " 'diast 8:00',\n",
    " 'diast 8:15',\n",
    " 'diast 8:30',\n",
    " 'diast 8:45']\n",
    "\n",
    "dados_sist_diast =  dataset[ls_sist_diast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tratar nulos\n",
    "dados_sist_diast_sem_nan = dados_sist_diast.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculando a quantidade ótima de clusters (é possivel dividir os pacientes em quantos grupos)\n",
    "sum_of_squares = calculate_wcss(dados_sist_diast_sem_nan)\n",
    "n = optimal_number_of_clusters(sum_of_squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertendo os dados para a clusterização\n",
    "X = np.array(dados_sist_diast_sem_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#habilitando o kmeans com 8 grupos (foi calculado na etapa anterior)\n",
    "kmeans = KMeans(n_clusters=n, random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listar os grupos \n",
    "grupos = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incluir grupos nos dados de sist e diast\n",
    "dados_sist_diast_sem_nan['grupo'] = grupos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O método Elbow\n",
    "\n",
    "### Esse método é utilizado para calcular o melhor número de cluster para o algoritmo, através do grafico.\n",
    "\n",
    "### https://minerandodados.com.br/algoritmo-k-means-python-passo-passo/#:~:text=O%20m%C3%A9todo%20Elbow%20se%20trata,um%20valor%20significativo%20de%20ganho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metodo de Elbow\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,20)\n",
    "\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(X)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Metodo Elbow Para Otimizar k')\n",
    "fig = plt.gcf() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('diastolica_sistolica/diast_sist_elbow.png', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhouette\n",
    "\n",
    "####  A análise da Silhouette pode ser usada para determinar o grau de separação entre os clusters. O coeficiente de Silhouette quando próximo de +1, indica que os pontos estão muito longe dos pontos do outro cluster, e quando próximo de 0, indica que os pontos então muito perto ou até interseccionando um outro cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Silhouette\n",
    "range_n_clusters = [2, 3, 4, 5, 6, 8]\n",
    "for n_clusters in range_n_clusters:    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "    # O 1st subplot é o grafico silhouette\n",
    "    # O coeficiente silhouette  um range de -1, 1 but in this example all\n",
    "    # Os exemplos vão ficar em [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # O (n_clusters + 1) * 10 é para inserir um espaço em branco entre a silhueta\n",
    "    # Para gerar destaque\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "    # Inicialize o clusterer com o valor n_clusters e um valor aleatorio\n",
    "    # seed 10.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "# O silhouette_score fornece o valor médio para todas as amostras.\n",
    "    # Isso dá uma perspectiva sobre a densidade e separação das formas\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"Para n_clusters =\", n_clusters,\n",
    "          \"O score_silhouette medio e :\", silhouette_avg)\n",
    "    # Calcule as pontuações da silhueta para cada amostra\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Agregue as pontuações de silhueta para amostras pertencentes a\n",
    "        # cluster i, e classifica-los\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color,       alpha=0.7)\n",
    "        # Rotule os gráficos de silhueta com seus números de cluster no meio\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        # Calcule o novo y_lower para o próximo gráfico\n",
    "        y_lower = y_upper + 10  #10 para as 0 amostras\n",
    "    ax1.set_title(\"O grafico de silhueta para os varios clusters.\")\n",
    "    ax1.set_xlabel(\"O valor silhouette coefficient\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "    # A linha vertical para a pontuação média da silhueta de todos os valores\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    ax1.set_yticks([])  # Limpar os yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    # 2º gráfico mostrando os clusters reais formados\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "    # Labeling dos clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Desenhe círculos brancos nos centros do cluster\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "    ax2.set_title(\"A visualizacao dos dados agrupados.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "    plt.suptitle((\"Analise de silhueta para agrupamento KMeans em dados de amostra\"\n",
    "                  \"com n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    fig = plt.gcf()\n",
    "    fig.savefig('diastolica_sistolica/diast_sist_kmeans_'+str(n_clusters)+'.png', format='png')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "#fig.savefig('diast_sist_kmeans.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não existe um valor de k clusters que os não esteja se intersectando "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset com grupos e por dias e sist\n",
    "dataset_s_d = dataset\n",
    "dataset_s_d['grupos'] = grupos\n",
    "dataset_s_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_s_d.to_csv('diastolica_sistolica/dataset_sistolica_diastolica.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parametro simples do modelo\n",
    "random_forest = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base para modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "qtds = []\n",
    "\n",
    "contagem = Counter(grupos)\n",
    "\n",
    "for i in range(0,n):\n",
    "    classes.append(i)\n",
    "    qtds.append(contagem[i])\n",
    "\n",
    "plt.bar(classes,qtds,color=\"red\")\n",
    "plt.xticks(classes)\n",
    "plt.ylabel('Quantidade de Pacientes')\n",
    "plt.xlabel('Grupo gerado por kmeans')\n",
    "plt.title('Numero pacientes x Grupos por kmeans (diast e sist)')\n",
    "fig = plt.gcf()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('diastolica_sistolica/qtd_x_grupos_dist_sist.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "#separar train(75%) - 14.485, test (10%) - 7.726  and validation (15%) - 11.588\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, grupos, test_size=1 - train_ratio,random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(X, grupos, test_size=test_ratio/(test_ratio + validation_ratio),random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treino\n",
    "random_forest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VALIDAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcular acuracia, recall e f1\n",
    "y_prev_val = random_forest.predict(x_val)\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_prev_val)\n",
    "recall = recall_score(y_val, y_prev_val,average='macro')\n",
    "f1 = f1_score(y_val, y_prev_val,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: '+str(accuracy)+' - Recall: '+str(recall)+' - F1 Score: '+str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de Confusão\n",
    "mat=list(confusion_matrix(y_val, y_prev_val, labels=classes))\n",
    "pd.DataFrame(mat,index = classes, columns = classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcular acuracia, recall e f1\n",
    "y_prev_test = random_forest.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_prev_test)\n",
    "recall = recall_score(y_test, y_prev_test,average='macro')\n",
    "f1 = f1_score(y_test, y_prev_test,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: '+str(accuracy)+' - Recall: '+str(recall)+' - F1 Score: '+str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz da Confusão\n",
    "mat=list(confusion_matrix(y_test, y_prev_test, labels=classes))\n",
    "pd.DataFrame(mat,index = classes, columns = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#salvar dataframe numero_paciente, grupo_real, grupo, previsto validação\n",
    "x_train_, x_test_, y_train_, y_test_ = train_test_split(dataset_s_d, grupos, test_size=1 - train_ratio, random_state=42)\n",
    "x_val_, x_test_, y_val_, y_test_ = train_test_split(dataset_s_d, grupos, test_size=test_ratio/(test_ratio + validation_ratio), random_state=42)\n",
    "#validacao\n",
    "pacientes_validacao_diast_sisto = x_val_[['numero_identificacao']]\n",
    "pacientes_validacao_diast_sisto['Grupo_Real'] = y_val_\n",
    "pacientes_validacao_diast_sisto['Grupo_Previsto'] = y_prev_val\n",
    "pacientes_validacao_diast_sisto.to_excel('diastolica_sistolica/pacientes_validacao_diast_sisto.xlsx')\n",
    "#teste\n",
    "pacientes_teste_diast_sisto = x_test_[['numero_identificacao']]\n",
    "pacientes_teste_diast_sisto['Grupo_Real'] = y_test_\n",
    "pacientes_teste_diast_sisto['Grupo_Previsto'] = y_prev_test\n",
    "pacientes_teste_diast_sisto.to_excel('diastolica_sistolica/pacientes_teste_diast_sisto.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculo da importancia de cada variavel para idenficar os grupos com 3 casas decimais\n",
    "importances = pd.DataFrame({'features':ls_sist_diast,'importance':np.round(random_forest.feature_importances_,3)})\n",
    "importances = importances.sort_values('importance',ascending=False).set_index('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances.to_excel('diastolica_sistolica/features_sisto_diasto.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medidas Estatisticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecionando variaveis estatisticas\n",
    "\n",
    "ls_estatistica = [\n",
    "    'Media Sistolica em 24h',\n",
    "    'Media Diastolica em 24h',\n",
    "    'Media Sistolica Diurna',\n",
    "    'Media Sistolica Matutina',\n",
    "    'Media Sistolica Vespertina',\n",
    "    'Media Sistolica Daytime',\n",
    "    'Media Sistolica Nighttime',\n",
    "    'Media Diastolica Diurna',\n",
    "    'Media Diastolica Matutina',\n",
    "    'Media Diastolica Vespertina',\n",
    "    'Media Diastolica Daytime',\n",
    "    'Media Diastolica Nighttime',\n",
    "    'Variancia Sistolica em 24h',\n",
    "    'Variancia Diastolica em 24h',\n",
    "    'Mediana Sistolica em 24h',\n",
    "    'Mediana Diastolica em 24h',\n",
    "    'AUC Sistolica em 24h',\n",
    "    'AUC Diastolica em 24h',\n",
    "    'AUC PP Daytime',\n",
    "    'AUC PP Nighttime',\n",
    "    'DP Sistolica em 24h',\n",
    "    'DP Diastolica em 24h',\n",
    "]\n",
    "\n",
    "dados_estatistica =  dataset[ls_estatistica]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tratar nulos\n",
    "dados_estatistica_sem_nan = dados_estatistica.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculando a quantidade ótima de clusters (é possivel dividir os pacientes em quantos grupos)\n",
    "sum_of_squares = calculate_wcss(dados_estatistica_sem_nan)\n",
    "n = optimal_number_of_clusters(sum_of_squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertendo os dados para a clusterização\n",
    "X = np.array(dados_estatistica_sem_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#habilitando o kmeans com 7 grupos (foi calculado na etapa anterior)\n",
    "kmeans = KMeans(n_clusters=n, random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listar os grupos \n",
    "grupos = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incluir grupos nos dados de sist e diast\n",
    "dados_estatistica_sem_nan['grupo'] = grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metodo Elbow\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,20)\n",
    "\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(X)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Metodo Elbow Para Otimizar k')\n",
    "fig = plt.gcf() \n",
    "fig.savefig('estatisticas/estatisticas_elbow.png', format='png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Silhouette\n",
    "range_n_clusters = [2, 3, 4, 5, 6, 7]\n",
    "for n_clusters in range_n_clusters:    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "    # O 1st subplot é o grafico silhouette\n",
    "    # O coeficiente silhouette  um range de -1, 1 but in this example all\n",
    "    # Os exemplos vão ficar em [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # O (n_clusters + 1) * 10 é para inserir um espaço em branco entre a silhueta\n",
    "    # Para gerar destaque\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "    # Inicialize o clusterer com o valor n_clusters e um valor aleatorio\n",
    "    # seed 10.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "# O silhouette_score fornece o valor médio para todas as amostras.\n",
    "    # Isso dá uma perspectiva sobre a densidade e separação das formas\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"Para n_clusters =\", n_clusters,\n",
    "          \"O score_silhouette medio e :\", silhouette_avg)\n",
    "    # Calcule as pontuações da silhueta para cada amostra\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Agregue as pontuações de silhueta para amostras pertencentes a\n",
    "        # cluster i, e classifica-los\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color,       alpha=0.7)\n",
    "        # Rotule os gráficos de silhueta com seus números de cluster no meio\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        # Calcule o novo y_lower para o próximo gráfico\n",
    "        y_lower = y_upper + 10  #10 para as 0 amostras\n",
    "    ax1.set_title(\"O grafico de silhueta para os varios clusters.\")\n",
    "    ax1.set_xlabel(\"O valor silhouette coefficient\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "    # A linha vertical para a pontuação média da silhueta de todos os valores\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    ax1.set_yticks([])  # Limpar os yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    # 2º gráfico mostrando os clusters reais formados\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "    # Labeling dos clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Desenhe círculos brancos nos centros do cluster\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "    ax2.set_title(\"A visualizacao dos dados agrupados.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "    plt.suptitle((\"Analise de silhueta para agrupamento KMeans em dados de amostra\"\n",
    "                  \"com n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig('estatisticas/estatisticas_kmeans_'+str(n_clusters)+'.png', format='png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#fig.savefig('kmeans_estatisticas.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset com grupos e por estatisticas\n",
    "dataset_estatistica = dataset\n",
    "dataset_estatistica['grupos'] = grupos\n",
    "dataset_estatistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_estatistica.to_csv('estatisticas/dataset_estatistica.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parametro simples do modelo\n",
    "random_forest = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base para modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "qtds = []\n",
    "\n",
    "contagem = Counter(grupos)\n",
    "\n",
    "for i in range(0,n):\n",
    "    classes.append(i)\n",
    "    qtds.append(contagem[i])\n",
    "\n",
    "plt.bar(classes,qtds,color=\"red\")\n",
    "plt.xticks(classes)\n",
    "plt.ylabel('Quantidade de Pacientes')\n",
    "plt.xlabel('Grupo gerado por kmeans')\n",
    "plt.title('Numero pacientes x Grupos por kmeans (estatisticas)')\n",
    "fig = plt.gcf()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#salvar a imagem\n",
    "fig.savefig('estatisticas/qtd_x_grupos_estatisticas.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "#separar train(75%) - 14.485, test (10%) - 7.726  and validation (15%) - 11.588\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, grupos, test_size=1 - train_ratio,random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(X, grupos, test_size=test_ratio/(test_ratio + validation_ratio),random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treino\n",
    "random_forest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VALIDAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcular acuracia, recall e f1\n",
    "y_prev_val = random_forest.predict(x_val)\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_prev_val)\n",
    "recall = recall_score(y_val, y_prev_val,average='macro')\n",
    "f1 = f1_score(y_val, y_prev_val,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: '+str(accuracy)+' - Recall: '+str(recall)+' - F1 Score: '+str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz da Confusão\n",
    "mat=list(confusion_matrix(y_val, y_prev_val, labels=classes))\n",
    "pd.DataFrame(mat,index = classes, columns = classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcular acuracia, recall e f1\n",
    "y_prev_test = random_forest.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_prev_test)\n",
    "recall = recall_score(y_test, y_prev_test,average='macro')\n",
    "f1 = f1_score(y_test, y_prev_test,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: '+str(accuracy)+' - Recall: '+str(recall)+' - F1 Score: '+str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz da Confusão\n",
    "mat=list(confusion_matrix(y_test, y_prev_test, labels=classes))\n",
    "pd.DataFrame(mat,index = classes, columns = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#salvar dataframe numero_paciente, grupo_real, grupo, previsto validação\n",
    "x_train_, x_test_, y_train_, y_test_ = train_test_split(dataset_estatistica, grupos, test_size=1 - train_ratio, random_state=42)\n",
    "x_val_, x_test_, y_val_, y_test_ = train_test_split(dataset_estatistica, grupos, test_size=test_ratio/(test_ratio + validation_ratio), random_state=42)\n",
    "#validacao\n",
    "pacientes_validacao_estatistica = x_val_[['numero_identificacao']]\n",
    "pacientes_validacao_estatistica['Grupo_Real'] = y_val_\n",
    "pacientes_validacao_estatistica['Grupo_Previsto'] = y_prev_val\n",
    "pacientes_validacao_estatistica.to_excel('estatisticas/pacientes_validacao_estatistica.xlsx')\n",
    "#teste\n",
    "pacientes_teste_estatistica = x_test_[['numero_identificacao']]\n",
    "pacientes_teste_estatistica['Grupo_Real'] = y_test_\n",
    "pacientes_teste_estatistica['Grupo_Previsto'] = y_prev_test\n",
    "pacientes_teste_estatistica.to_excel('estatisticas/pacientes_teste_estatistica.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculo da importancia de cada variavel para idenficar os grupos com 3 casas decimais\n",
    "importances = pd.DataFrame({'features':ls_estatistica,'importance':np.round(random_forest.feature_importances_,3)})\n",
    "importances = importances.sort_values('importance',ascending=False).set_index('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances.to_excel('estatisticas/features_estatisticas.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padrão de Doencas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecionando padrões de doencas\n",
    "#dataset.iloc[:,172:183].columns\n",
    "\n",
    "ls_padroes_doencas = ['Hipotensao', 'Sistolica Isolada', 'Diastolica Isolada', 'Dipping',\n",
    "       'Non Dipping', 'Extreme Dipping', 'Reverse Dipping', 'Morning Surge',\n",
    "       'Masked', 'Whitecoat', 'Normotenso']\n",
    "\n",
    "dados_padrao_doencas =  dataset[ls_padroes_doencas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tratar nulos\n",
    "dados_padrao_doencas_sem_nan = dados_padrao_doencas.fillna(0)\n",
    "dados_padrao_doencas_sem_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trocar nulos do dataframe\n",
    "d_p_d = dados_padrao_doencas_sem_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trocar True e False para 1 e 0\n",
    "d_p_d['Hipotensao'] = d_p_d['Hipotensao'].apply(verdadeiro_falso)\n",
    "d_p_d['Sistolica Isolada'] = d_p_d['Sistolica Isolada'].apply(verdadeiro_falso)\n",
    "d_p_d['Diastolica Isolada'] = d_p_d['Diastolica Isolada'].apply(verdadeiro_falso)\n",
    "d_p_d['Dipping'] = d_p_d['Dipping'].apply(verdadeiro_falso)\n",
    "d_p_d['Non Dipping'] = d_p_d['Non Dipping'].apply(verdadeiro_falso)\n",
    "d_p_d['Extreme Dipping'] = d_p_d['Extreme Dipping'].apply(verdadeiro_falso)\n",
    "d_p_d['Reverse Dipping'] = d_p_d['Reverse Dipping'].apply(verdadeiro_falso)\n",
    "d_p_d['Morning Surge'] = d_p_d['Morning Surge'].apply(verdadeiro_falso)\n",
    "d_p_d['Masked'] = d_p_d['Masked'].apply(verdadeiro_falso)\n",
    "d_p_d['Whitecoat'] = d_p_d['Whitecoat'].apply(verdadeiro_falso)\n",
    "d_p_d['Normotenso'] = d_p_d['Normotenso'].apply(verdadeiro_falso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculando a quantidade ótima de clusters (é possivel dividir os pacientes em quantos grupos)\n",
    "sum_of_squares = calculate_wcss(d_p_d)\n",
    "n = optimal_number_of_clusters(sum_of_squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertendo os dados para a clusterização\n",
    "X = np.array(d_p_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#habilitando o kmeans com 7 grupos (foi calculado na etapa anterior)\n",
    "kmeans = KMeans(n_clusters=n, random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listar os grupos \n",
    "grupos = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incluir grupos nos dados de padrão de doença\n",
    "dados_padrao_doencas =  dataset[ls_padroes_doencas]\n",
    "dados_padrao_doencas_sem_nan = dados_padrao_doencas.fillna(0)\n",
    "dados_padrao_doencas_sem_nan['grupo'] = grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metodo Elbow\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,20)\n",
    "\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(X)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Metodo Elbow Para Otimizar k')\n",
    "fig = plt.gcf()\n",
    "fig.savefig('padrao_doencas/padrao_doencas_elbow.png', format='png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Silhouette\n",
    "range_n_clusters = [2, 3, 4, 5, 6, 7]\n",
    "for n_clusters in range_n_clusters:    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "    # O 1st subplot é o grafico silhouette\n",
    "    # O coeficiente silhouette  um range de -1, 1 but in this example all\n",
    "    # Os exemplos vão ficar em [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # O (n_clusters + 1) * 10 é para inserir um espaço em branco entre a silhueta\n",
    "    # Para gerar destaque\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "    # Inicialize o clusterer com o valor n_clusters e um valor aleatorio\n",
    "    # seed 10.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "# O silhouette_score fornece o valor médio para todas as amostras.\n",
    "    # Isso dá uma perspectiva sobre a densidade e separação das formas\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"Para n_clusters =\", n_clusters,\n",
    "          \"O score_silhouette medio e :\", silhouette_avg)\n",
    "    # Calcule as pontuações da silhueta para cada amostra\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Agregue as pontuações de silhueta para amostras pertencentes a\n",
    "        # cluster i, e classifica-los\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color,       alpha=0.7)\n",
    "        # Rotule os gráficos de silhueta com seus números de cluster no meio\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        # Calcule o novo y_lower para o próximo gráfico\n",
    "        y_lower = y_upper + 10  #10 para as 0 amostras\n",
    "    ax1.set_title(\"O grafico de silhueta para os varios clusters.\")\n",
    "    ax1.set_xlabel(\"O valor silhouette coefficient\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "    # A linha vertical para a pontuação média da silhueta de todos os valores\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    ax1.set_yticks([])  # Limpar os yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    # 2º gráfico mostrando os clusters reais formados\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "    # Labeling dos clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Desenhe círculos brancos nos centros do cluster\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "    ax2.set_title(\"A visualizacao dos dados agrupados.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "    plt.suptitle((\"Analise de silhueta para agrupamento KMeans em dados de amostra\"\n",
    "                  \"com n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig('padrao_doencas/padrao_doencas_kmeans_'+str(n_clusters)+'.png', format='png')  \n",
    "\n",
    "plt.show()\n",
    "\n",
    "#fig.savefig('kmeans_padroes_doencas.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset com grupos e por padrão de doencas\n",
    "dataset_padrao_doenca = dataset\n",
    "dataset_padrao_doenca['grupos'] = grupos\n",
    "dataset_padrao_doenca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_padrao_doenca.to_csv('padrao_doencas/dataset_padrao_doenca.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parametro simples do modelo\n",
    "random_forest = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base para modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "qtds = []\n",
    "\n",
    "contagem = Counter(grupos)\n",
    "\n",
    "for i in range(0,n):\n",
    "    classes.append(i)\n",
    "    qtds.append(contagem[i])\n",
    "\n",
    "plt.bar(classes,qtds,color=\"red\")\n",
    "plt.xticks(classes)\n",
    "plt.ylabel('Quantidade de Pacientes')\n",
    "plt.xlabel('Grupo gerado por kmeans')\n",
    "plt.title('Numero pacientes x Grupos por kmeans (Padrao de Doencas)')\n",
    "fig = plt.gcf()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#salvar a imagem\n",
    "fig.savefig('padrao_doencas/qtd_x_padrao_doencas.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "#separar train(75%) - 14.485, test (10%) - 7.726  and validation (15%) - 11.588\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, grupos, test_size=1 - train_ratio,random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(X, grupos, test_size=test_ratio/(test_ratio + validation_ratio),random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treino\n",
    "random_forest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VALIDAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcular acuracia, recall e f1\n",
    "y_prev_val = random_forest.predict(x_val)\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_prev_val)\n",
    "recall = recall_score(y_val, y_prev_val,average='macro')\n",
    "f1 = f1_score(y_val, y_prev_val,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: '+str(accuracy)+' - Recall: '+str(recall)+' - F1 Score: '+str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de Confusão\n",
    "mat=list(confusion_matrix(y_val, y_prev_val, labels=classes))\n",
    "pd.DataFrame(mat,index = classes, columns = classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcular acuracia, recall e f1\n",
    "y_prev_test = random_forest.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_prev_test)\n",
    "recall = recall_score(y_test, y_prev_test,average='macro')\n",
    "f1 = f1_score(y_test, y_prev_test,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: '+str(accuracy)+' - Recall: '+str(recall)+' - F1 Score: '+str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de Confusão\n",
    "mat=list(confusion_matrix(y_test, y_prev_test, labels=classes))\n",
    "pd.DataFrame(mat,index = classes, columns = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#salvar dataframe numero_paciente, grupo_real, grupo, previsto validação e teste\n",
    "x_train_, x_test_, y_train_, y_test_ = train_test_split(dataset_padrao_doenca, grupos, test_size=1 - train_ratio, random_state=42)\n",
    "x_val_, x_test_, y_val_, y_test_ = train_test_split(dataset_padrao_doenca, grupos, test_size=test_ratio/(test_ratio + validation_ratio), random_state=42)\n",
    "#validacao\n",
    "pacientes_validacao_padrao_doencas = x_val_[['numero_identificacao']]\n",
    "pacientes_validacao_padrao_doencas['Grupo_Real'] = y_val_\n",
    "pacientes_validacao_padrao_doencas['Grupo_Previsto'] = y_prev_val\n",
    "pacientes_validacao_padrao_doencas.to_excel('padrao_doencas/pacientes_validacao_padrao_doencas.xlsx')\n",
    "#teste\n",
    "pacientes_teste_padrao_doencas = x_test_[['numero_identificacao']]\n",
    "pacientes_teste_padrao_doencas['Grupo_Real'] = y_test_\n",
    "pacientes_teste_padrao_doencas['Grupo_Previsto'] = y_prev_test\n",
    "pacientes_teste_padrao_doencas.to_excel('padrao_doencas/pacientes_teste_padrao_doencas.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculo da importancia de cada variavel para idenficar os grupos com 3 casas decimais\n",
    "importances = pd.DataFrame({'features':ls_padroes_doencas,'importance':np.round(random_forest.feature_importances_,3)})\n",
    "importances = importances.sort_values('importance',ascending=False).set_index('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
